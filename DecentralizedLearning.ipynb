{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DecentralizedLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNc4kf1oDBNj70mpd7OpFf9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harikuts/federated-learning-trials/blob/master/DecentralizedLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cwDfXm8MvnQ",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This notebook contains the reproduction of results of the original paper on federated learning.\n",
        "\n",
        "## Plan\n",
        "\n",
        "The roadmap for development is as follows:\n",
        "*   Construct standard MNIST example.\n",
        "*   To be continued.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yo88uRHNvJq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "973a984c-caea-44f6-ba05-84a8205e8ca7"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "# !pip install tensorflow==2.1-rc0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0-rc0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyPkp1yZz527",
        "colab_type": "text"
      },
      "source": [
        "# Standard MNIST\n",
        "\n",
        "There are baseline implementations of a standard example of MNIST. A Keras implementation staands as the first example, but we will port this over to Tensorflow as it provides more low-level functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D12oVhOOc1gp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "outputId": "e1e4547e-4935-46e6-f6b7-bb04f7d59ad1"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "# Import MNIST data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Create model\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(32, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Predictions\n",
        "predictions = model(x_train[:1]).numpy()\n",
        "# Softmax\n",
        "tf.nn.softmax(predictions).numpy()\n",
        "\n",
        "# Defining the loss function\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "loss_fn(y_train[:1], predictions).numpy()\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'], validation_data=(x_test, y_test))\n",
        "\n",
        "# Fit model\n",
        "model.fit(x_train, y_train, epochs=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer flatten_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e37636938962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Compile model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0munknown_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m       raise TypeError(\n\u001b[0;32m--> 312\u001b[0;31m           'Invalid keyword argument(s) in `compile`: %s' % (unknown_kwargs,))\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid keyword argument(s) in `compile`: {'validation_data'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce4iHoknf_cE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a6f95f24-47e1-419d-e9e3-ae1b4b89a600"
      },
      "source": [
        "# print(model.get_weights()[0].shape)\n",
        "# print(model.get_weights()[1].shape)\n",
        "# print(model.get_weights()[2].shape)\n",
        "# print(model.get_weights()[3].shape)\n",
        "\n",
        "import numpy as np\n",
        "a = np.array([1, 2, 3, 4])\n",
        "\n",
        "b = a + a\n",
        "b = sum([a,a])\n",
        "print(b)\n",
        "b = b / 2\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 4 6 8]\n",
            "[1. 2. 3. 4.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXAJn845_4rV",
        "colab_type": "text"
      },
      "source": [
        "# Experimental Approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucr9Yx4a8Qtu",
        "colab_type": "text"
      },
      "source": [
        "## Federated Learning Validation\n",
        "\n",
        "### Network Model\n",
        "Here we use nodes to carry models. The reason for doing this to prevent the instantiation of new models each time weights have to be transferred. Instead, the state of each model can be preserved in the node that it resides in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WJjvFBBEYs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pdb\n",
        "\n",
        "# Used to start execution ASAP\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "# Configuration\n",
        "num_clients = 8\n",
        "num_epochs = 2\n",
        "num_server_rounds = 8\n",
        "num_client_rounds = 4\n",
        "nonIID = False\n",
        "print (\"Configuration:\" + \\\n",
        "       \"\\n\\t%d clients.\" % (num_clients) + \\\n",
        "       \"\\n\\t%d training epochs.\" % (num_epochs)  + \\\n",
        "       \"\\n\\tUsing %sIID data.\" % (\"non-\" if nonIID else \"\"))\n",
        "\n",
        "# Server class\n",
        "class Server:\n",
        "  def __init__(self, modelGenerator):\n",
        "    self.model = modelGenerator()\n",
        "    self.clients = []\n",
        "    self.neighbors = []\n",
        "# Client class\n",
        "class Client:\n",
        "  def __init__(self, modelGenerator):\n",
        "    self.model = modelGenerator()\n",
        "    self.neighbors = []\n",
        "    self.x_data = None\n",
        "    self.y_data = None\n",
        "    self.data_size = None\n",
        "  def plotAccuracy(self, histories):\n",
        "    # Compile histories\n",
        "    categorical_accuracy = []\n",
        "    val_categorical_accuracy = []\n",
        "    for history in histories:\n",
        "      categorical_accuracy = categorical_accuracy + history.history['acc']\n",
        "      # val_categorical_accuracy = val_categorical_accuracy + history.history['val_categorical_accuracy']\n",
        "    # The history of our accuracy during training.\n",
        "    plt.plot(categorical_accuracy)\n",
        "    plt.plot(val_categorical_accuracy)\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Number of epochs')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    return plt\n",
        "  def train(self):\n",
        "    history = self.model.fit(self.x_data, self.y_data, epochs=num_epochs)\n",
        "    # print(history.history.keys())\n",
        "    # self.accPlot = self.plotAccuracy([history])\n",
        "\n",
        "# NN model generator function\n",
        "def createNN():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10)\n",
        "  ])\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "  return model\n",
        "# Weight averaging\n",
        "def averageWeights(weightsList, weighting=None):\n",
        "  denominator = len(weightsList)\n",
        "  new_weights = []\n",
        "  if weighting is None:\n",
        "    # Handle IID data (balanced)\n",
        "    for part in range(len(weightsList[0])):\n",
        "      part_stack = [weights[part] for weights in weightsList]\n",
        "      new_stack = sum(part_stack) / denominator\n",
        "      new_stack = np.array(new_stack)\n",
        "      new_weights.append(new_stack)\n",
        "    return new_weights\n",
        "  else:\n",
        "    for part in range(len(weightsList[0])):\n",
        "      part_stack = [weights[part] for weights in weightsList]\n",
        "      # part_stack = np.array(part_stack) * weighting\n",
        "      for i in range(len(weighting)):\n",
        "        part_stack[i] = part_stack[i] * weighting[i]\n",
        "      new_stack = sum(part_stack)\n",
        "      new_stack = np.array(new_stack)\n",
        "      new_weights.append(new_stack)\n",
        "    return new_weights\n",
        "\n",
        "\n",
        "# Create the network\n",
        "print (\"\\nCreating a network...\")\n",
        "server = Server(createNN)\n",
        "for i in range(num_clients):\n",
        "  server.clients.append(Client(createNN))\n",
        "\n",
        "# Import MNIST data\n",
        "print (\"\\nDownloading MNIST data...\")\n",
        "mnist = tf.keras.datasets.mnist\n",
        "# Load data into trains\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Splitting the dataset for different clients\n",
        "print (\"\\nSplitting data into different clients...\")\n",
        "if nonIID:\n",
        "  print (\"\\tRandomly assigning ranges of data...\")\n",
        "  percentageMarkers = []\n",
        "  for i in range(num_clients-1):\n",
        "    percentageMarkers.append(random.random())\n",
        "  percentageMarkers.append(1.0)\n",
        "  percentageMarkers = sorted(percentageMarkers)\n",
        "else:\n",
        "  print (\"\\tUniformly assigning ranges of data\")\n",
        "  percentageMarkers = [1/num_clients * (n+1) for n in range(num_clients)]\n",
        "# Storing each subset of data in a client\n",
        "print (\"\\tStoring subsets of data into each client...\")\n",
        "xMarkers = [int(marker * len(x_train)) for marker in percentageMarkers]\n",
        "yMarkers = [int(marker * len(y_train)) for marker in percentageMarkers]\n",
        "for j in range(len(percentageMarkers)):\n",
        "  server.clients[j].x_data = x_train[(xMarkers[j-1] if j > 0 else 0):xMarkers[j]]\n",
        "  server.clients[j].y_data = y_train[(yMarkers[j-1] if j > 0 else 0):yMarkers[j]]\n",
        "  server.clients[j].data_size = len(server.clients[j].x_data)\n",
        "\n",
        "# Client data diagnostic\n",
        "print (\"\\nFinished setting up client data!\")\n",
        "for client in server.clients:\n",
        "  print (\"\\tClient %d:\\tX: %d\\tY: %d\" % (server.clients.index(client), len(client.x_data), len(client.y_data)))\n",
        "\n",
        "# Server action\n",
        "server_accuracies = []\n",
        "server_losses = []\n",
        "for server_round in range(num_server_rounds):\n",
        "  print(\"\\nSERVER ROUND \", server_round, \":\\n\")\n",
        "  # Save server model weights\n",
        "  global_weights = server.model.get_weights()\n",
        "  # Clients' actions\n",
        "  client_weight_list = []\n",
        "  for client in server.clients:\n",
        "    print(\"\\nCLIENT \", server.clients.index(client), \":\\n\")\n",
        "    # Initialize recorded weights\n",
        "    round_weight_list = []\n",
        "    for client_round in range(num_client_rounds):\n",
        "      # Accept global weights\n",
        "      client.model.set_weights(global_weights)\n",
        "      # Train\n",
        "      client.train()\n",
        "      # Record weights\n",
        "      round_weight_list.append(client.model.get_weights())\n",
        "    client_weight_list.append(averageWeights(round_weight_list))\n",
        "  client_data_sizes = [client.data_size for client in server.clients]\n",
        "  client_weighting = np.array(client_data_sizes) / sum(client_data_sizes)\n",
        "  server.model.set_weights(averageWeights(client_weight_list, weighting=client_weighting))\n",
        "  loss, acc = server.model.evaluate(x_test, y_test)\n",
        "  print(\"\\nSERVER ROUND \", server_round, \" ACCURACY: \", acc, \"\\n\")\n",
        "  server_accuracies.append(acc)\n",
        "  server_losses.append(loss)\n",
        "  print(\"FINAL RESULTS:\\nAccuracies: \", server_accuracies, \"\\nLoss: \", server_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFNtu_EoZI9q",
        "colab_type": "text"
      },
      "source": [
        "## Decentralized Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFxk_AzpZObH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pdb\n",
        "\n",
        "# Used to start execution ASAP\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "# Configuration\n",
        "num_clients = 8\n",
        "num_epochs = 2\n",
        "num_learning_rounds = 16\n",
        "num_client_rounds = 1\n",
        "link_reliability = 0.75\n",
        "nonIID = False\n",
        "print (\"Configuration:\" + \\\n",
        "       \"\\n\\t%d clients.\" % (num_clients) + \\\n",
        "       \"\\n\\t%d training epochs.\" % (num_epochs)  + \\\n",
        "       \"\\n\\tUsing %sIID data.\" % (\"non-\" if nonIID else \"\"))\n",
        "\n",
        "# Client class\n",
        "class Client:\n",
        "  def __init__(self, modelGenerator):\n",
        "    self.model = modelGenerator()\n",
        "    self.neighbors = []\n",
        "    self.x_data = None\n",
        "    self.y_data = None\n",
        "    self.data_size = None\n",
        "    self.accuracy_history = []\n",
        "    self.loss_history = []\n",
        "  def plotAccuracy(self, histories):\n",
        "    # Compile histories\n",
        "    categorical_accuracy = []\n",
        "    val_categorical_accuracy = []\n",
        "    for history in histories:\n",
        "      categorical_accuracy = categorical_accuracy + history.history['acc']\n",
        "    # The history of our accuracy during training.\n",
        "    plt.plot(categorical_accuracy)\n",
        "    plt.plot(val_categorical_accuracy)\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Number of epochs')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    return plt\n",
        "  def train(self):\n",
        "    history = self.model.fit(self.x_data, self.y_data, epochs=num_epochs)\n",
        "    self.setOutgoingMessage()\n",
        "    # print(history.history.keys())\n",
        "    # self.accPlot = self.plotAccuracy([history])\n",
        "  def test(self, x, y):\n",
        "    loss, acc = self.model.evaluate(x, y, verbose=1)\n",
        "    self.accuracy_history.append(acc)\n",
        "    self.loss_history.append(loss)\n",
        "    return loss, acc\n",
        "  def setOutgoingMessage(self):\n",
        "    self.outgoing_message = (self.data_size, self.model.get_weights())\n",
        "  def communityLearn(self):\n",
        "    # Accept incoming messages\n",
        "    incoming_messages = []\n",
        "    for neighbor in self.neighbors:\n",
        "      # Link reliability functionality\n",
        "      r = random.random()\n",
        "      if (r <= link_reliability) or (clientList.index(neighbor) == clientList.index(self)):\n",
        "        incoming_messages.append(neighbor.outgoing_message)\n",
        "    # Grab sizes, set ratios, grab weights\n",
        "    ordered_sizes = [message[0] for message in incoming_messages]\n",
        "    ordered_sizes = np.array(ordered_sizes) / sum(ordered_sizes)\n",
        "    ordered_weights = [message[1] for message in incoming_messages]\n",
        "    # Compute new weights\n",
        "    new_weights = averageWeights(ordered_weights, ordered_sizes)\n",
        "    # Create new model with appropriate weights\n",
        "    self.model = createNN()\n",
        "    self.model.set_weights(new_weights)\n",
        "\n",
        "# NN model generator function\n",
        "def createNN():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10)\n",
        "  ])\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "  return model\n",
        "# Weight averaging\n",
        "def averageWeights(weightsList, weighting=None):\n",
        "  denominator = len(weightsList)\n",
        "  new_weights = []\n",
        "  if weighting is None:\n",
        "    # Handle IID data (balanced)\n",
        "    for part in range(len(weightsList[0])):\n",
        "      part_stack = [weights[part] for weights in weightsList]\n",
        "      new_stack = sum(part_stack) / denominator\n",
        "      new_stack = np.array(new_stack)\n",
        "      new_weights.append(new_stack)\n",
        "    return new_weights\n",
        "  else:\n",
        "    for part in range(len(weightsList[0])):\n",
        "      part_stack = [weights[part] for weights in weightsList]\n",
        "      # part_stack = np.array(part_stack) * weighting\n",
        "      for i in range(len(weighting)):\n",
        "        part_stack[i] = part_stack[i] * weighting[i]\n",
        "      new_stack = sum(part_stack)\n",
        "      new_stack = np.array(new_stack)\n",
        "      new_weights.append(new_stack)\n",
        "    return new_weights\n",
        "\n",
        "\n",
        "# # Create a strongly connected network\n",
        "# print (\"\\nCreating a network...\")\n",
        "# clientList = []\n",
        "# for i in range(num_clients):\n",
        "#   clientList.append(Client(createNN))\n",
        "# # Add neighbors\n",
        "# for client in clientList:\n",
        "#   for neighbor in clientList:\n",
        "#     client.neighbors.append(neighbor)\n",
        "\n",
        "# Create a weakly connected network\n",
        "print (\"\\nCreating a weakly connected network...\")\n",
        "clientList = []\n",
        "for i in range(8):\n",
        "  clientList.append(Client(createNN))\n",
        "# Add linkings\n",
        "clientList[0].neighbors += [clientList[0], clientList[2], clientList[3], clientList[4]]\n",
        "clientList[1].neighbors += [clientList[1], clientList[2]]\n",
        "clientList[2].neighbors += [clientList[0], clientList[1], clientList[2], clientList[3]]\n",
        "clientList[3].neighbors += [clientList[0], clientList[2], clientList[3], clientList[4], clientList[7]]\n",
        "clientList[4].neighbors += [clientList[0], clientList[3], clientList[4], clientList[5], clientList[6], clientList[7]]\n",
        "clientList[5].neighbors += [clientList[4], clientList[5], clientList[6]]\n",
        "clientList[6].neighbors += [clientList[4], clientList[5], clientList[6], clientList[7]]\n",
        "clientList[7].neighbors += [clientList[3], clientList[4], clientList[6], clientList[7]]\n",
        "\n",
        "# Import MNIST data\n",
        "print (\"\\nDownloading MNIST data...\")\n",
        "mnist = tf.keras.datasets.mnist\n",
        "# Load data into trains\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Splitting the dataset for different clients\n",
        "print (\"\\nSplitting data into different clients...\")\n",
        "if nonIID:\n",
        "  print (\"\\tRandomly assigning ranges of data...\")\n",
        "  percentageMarkers = []\n",
        "  for i in range(num_clients-1):\n",
        "    percentageMarkers.append(random.random())\n",
        "  percentageMarkers.append(1.0)\n",
        "  percentageMarkers = sorted(percentageMarkers)\n",
        "else:\n",
        "  print (\"\\tUniformly assigning ranges of data\")\n",
        "  percentageMarkers = [1/num_clients * (n+1) for n in range(num_clients)]\n",
        "# Storing each subset of data in a client\n",
        "print (\"\\tStoring subsets of data into each client...\")\n",
        "xMarkers = [int(marker * len(x_train)) for marker in percentageMarkers]\n",
        "yMarkers = [int(marker * len(y_train)) for marker in percentageMarkers]\n",
        "for j in range(len(percentageMarkers)):\n",
        "  clientList[j].x_data = x_train[(xMarkers[j-1] if j > 0 else 0):xMarkers[j]]\n",
        "  clientList[j].y_data = y_train[(yMarkers[j-1] if j > 0 else 0):yMarkers[j]]\n",
        "  clientList[j].data_size = len(clientList[j].x_data)\n",
        "\n",
        "# Client data diagnostic\n",
        "print (\"\\nFinished setting up client data!\")\n",
        "for client in clientList:\n",
        "  print (\"\\tClient %d:\\tX: %d\\tY: %d\" % (clientList.index(client), len(client.x_data), len(client.y_data)))\n",
        "\n",
        "for learning_round in range(num_learning_rounds):\n",
        "  print(\"\\nLEARNING ROUND \", learning_round, \":\\n\")\n",
        "  # Have each client learn on its data\n",
        "  for client in clientList:\n",
        "    print (\"\\nROUND\", learning_round, \", CLIENT\", clientList.index(client), \"TRAINING\\n\")\n",
        "    client.train()\n",
        "  # Communicate and learn\n",
        "  for client in clientList:\n",
        "    print (\"\\nROUND\", learning_round, \", CLIENT\", clientList.index(client), \"LEARNING\\n\")\n",
        "    client.communityLearn()\n",
        "  # Test at the end of this round\n",
        "  for client in clientList:\n",
        "    print (\"\\nROUND\", learning_round, \", CLIENT\", clientList.index(client), \"TESTING\\n\")\n",
        "    client.test(x_test, y_test)\n",
        "# Print out results\n",
        "for client in clientList:\n",
        "  print(\"Client \", clientList.index(client), \":\\n\")\n",
        "  print(\"\\tAccuracy History: \", client.accuracy_history, \"\\n\")\n",
        "  print(\"\\tLoss History: \", client.loss_history, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}